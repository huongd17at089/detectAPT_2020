{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Graph2Vec.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"pnyfg-VR_kiW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605084324509,"user_tz":-420,"elapsed":1788,"user":{"displayName":"chan nene","photoUrl":"","userId":"05810351668712723302"}},"outputId":"e1df6b4c-9cf9-4f5f-f549-de5d558f0ce4"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hCSLaW4cAwpE"},"source":["# constant"]},{"cell_type":"code","metadata":{"id":"LUn9YtzFAft7"},"source":["data_folder  = \"/content/drive/My Drive/Graph2Vec/data/\"\n","workers = 4\n","wl_iterations = 2\n","dimensions = 64\n","min_count = 1\n","down_sampling = 0.0001\n","epochs = 100\n","learning_rate = 0.01\n","output_path = \"/content/drive/My Drive/Graph2Vec/vectors/vector.csv\"\n","# saved model \n","# \"/content/drive/My Drive/Graph2Vec/model/model\" \n","seve_model_path = \"/content/drive/My Drive/Graph2Vec/model/model1\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KGQqhEl8QQFb"},"source":["# WeisfeilerLehman"]},{"cell_type":"code","metadata":{"id":"jVVLCR4qAvBB"},"source":["import hashlib\n","class WeisfeilerLehman:\n","\n","  def __init__(self, graph, features, iterations):\n","\n","    self.iterations = iterations\n","    self.graph = graph\n","    self.features = features\n","    self.nodes = self.graph.nodes()\n","    self.extracted_features = [str(v) for k, v in features.items()]\n","    self.do_recursions()\n","\n","  def do_a_recursion(self):\n","\n","    new_features = {}\n","    for node in self.nodes:\n","      nebs = self.graph.neighbors(node)\n","      degs = [self.features[neb] for neb in nebs]\n","      features = [str(self.features[node])]+sorted([str(deg) for deg in degs])\n","      features = \"_\".join(features)\n","      hash_object = hashlib.md5(features.encode())\n","      hashing = hash_object.hexdigest()\n","      new_features[node] = hashing\n","    self.extracted_features = self.extracted_features + list(new_features.values())\n","    return new_features\n","\n","  def do_recursions(self):\n","\n","    for _ in range(self.iterations):\n","      self.features = self.do_a_recursion()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WAIPaiqzBQhB"},"source":["# \r\n"]},{"cell_type":"code","metadata":{"id":"3pPXM5NJBOap"},"source":["import json\n","import networkx as nx\n","import pandas as pd\n","def dataset_reader(path):\n","   name = path.strip(\".json\").split(\"/\")[-1]\n","    data = json.load(open(path, encoding=\"utf8\"))\n","    # print(path)\n","    edges = []\n","    features = {}\n","    if(len(data[\"Processes\"]) > 0) :\n","        incidents = {}\n","        incidents_list = data[\"Incidents\"]\n","        for inc in incidents_list:\n","            if (inc[\"MitreAttacks\"] is None):\n","                continue\n","            else:\n","                attacks = \"\"\n","                for a in inc[\"MitreAttacks\"]:\n","                    attacks += \" \" + a\n","                if (inc[\"ProcessOID\"] in incidents):\n","                    incidents[inc[\"ProcessOID\"]] = incidents.get(inc[\"ProcessOID\"]) + \" \" + attacks\n","                else:\n","                    incidents.update({inc[\"ProcessOID\"]: \"\"})\n","\n","        features.update({\"CommandLine\": {}})\n","        features.update({\"Image\": {}})\n","        features.update({\"Incidents\": incidents})\n","        features.update({\"ProcessType\": {}})\n","\n","        for p in data[\"Processes\"]:\n","            if (p[\"ProcessType\"] == \"Child process\"):\n","                edges.append([p[\"ParentPID\"], p[\"ProcessID\"]])\n","            if (p[\"ProcessID\"] not in incidents):\n","                features[\"Incidents\"].update({p[\"ProcessID\"]: \"\"})\n","            features[\"CommandLine\"].update({p[\"ProcessID\"]: p[\"CommandLine\"]})\n","            features[\"Image\"].update({p[\"ProcessID\"]: p[\"Image\"]})\n","            features[\"ProcessType\"].update({p[\"ProcessID\"]: p[\"ProcessType\"]})\n","            \n","  graph = nx.Graph(edges)\n","  return graph, features, name\n","\n","def feature_extractor(path, rounds):\n","  graph, features, name = dataset_reader(path)\n","  words_List = []\n","  for key in features:\n","    words_List = words_List + WeisfeilerLehman(graph, features[key], rounds).extracted_features\n","  doc = TaggedDocument(words=words_List, tags=[\"g_\" + name])\n","  return doc\n","\n","def save_to_csv(output_path, model, files, dimensions):\n","  out = []\n","    for f in files:\n","        identifier = f.split(\"/\")[-1].strip(\".json\")\n","        vector = [identifier] + list(model.docvecs[\"g_\"+identifier])\n","        if(\"malware\" in identifier) :\n","            vector  = vector  + [1]\n","        else:\n","            vector  = vector  + [0]\n","        out.append(vector)\n","    column_names = [\"id\"]+[\"x_\"+str(dim) for dim in range(dimensions)] + [\"label\"]\n","    out = pd.DataFrame(out, columns=column_names)\n","    out = out.sort_values([\"id\"])\n","    out.to_csv(output_path, index=None)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xuxqLlxYBUrx"},"source":["# main"]},{"cell_type":"code","metadata":{"id":"3zX6XcAqBW3p"},"source":["from tqdm import tqdm\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from joblib import Parallel, delayed\n","import glob\n","from gensim.test.utils import get_tmpfile\n","import os\n","\n","# graphs = glob.glob(f\"/content/drive/My Drive/Graph2Vec/malware/**/*.json\") + glob.glob(f\"/content/drive/My Drive/Graph2Vec/benign/**/*.json\")\n","graphs = []\n","for r, d, f in os.walk(data_folder):\n","  for file in f:\n","    if '.json' in file:\n","      graphs.append(os.path.join(r, file))\n","\n","document_collections = Parallel(n_jobs=workers)(delayed(feature_extractor)(g, wl_iterations) for g in tqdm(graphs))\n","\n","model = Doc2Vec(document_collections,\n","                    vector_size=dimensions,\n","                    window=2,\n","                    min_count=min_count,\n","                    dm=0,\n","                    sample=down_sampling,\n","                    workers=workers,\n","                    epochs=epochs,\n","                    alpha=learning_rate,\n","                    negative = 5\n","                    )\n","# fname = get_tmpfile(\"test_model\")\n","model.save(seve_model_path)\n","\n","save_to_csv(output_path, model, graphs, dimensions)"],"execution_count":null,"outputs":[]}]}